


import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

# Add the parent directory to the path to import our modules
sys.path.append('..')

# Import our modules
from src.preprocessing import load_adult_dataset, preprocess_adult_dataset
from src.clustering import MultiClusteringAlgorithm
from src.cmi import calculate_cmi, calculate_cmi_per_cluster, hierarchical_cmi_decomposition, interaction_information
from src.validation import permutation_test, bootstrap_ci, plot_permutation_test, plot_bootstrap_distribution
from src.mitigation import reweighting, FairnessRegularizedModel, subgroup_calibration, evaluate_mitigation, plot_mitigation_comparison





# Download Adult dataset if not already available
import urllib.request
import os

data_dir = '../data'
adult_data_path = os.path.join(data_dir, 'adult.data')

if not os.path.exists(adult_data_path):
    print("Downloading Adult dataset...")
    os.makedirs(data_dir, exist_ok=True)
    urllib.request.urlretrieve(
        'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',
        adult_data_path
    )
    print("Download complete.")

# Load the dataset
adult_data = load_adult_dataset(adult_data_path)

# Display basic info
print(f"Dataset shape: {adult_data.shape}")
print(adult_data.head())

# Preprocess the dataset
processed_data, sensitive_columns, nonsensitive_columns, outcome_column = preprocess_adult_dataset(adult_data)

print(f"Processed dataset shape: {processed_data.shape}")
print(f"Sensitive columns: {sensitive_columns}")
print(f"Outcome column: {outcome_column}")





# Look at distributions of sensitive attributes
fig, axes = plt.subplots(1, len(sensitive_columns) + 1, figsize=(15, 5))

for i, col in enumerate(sensitive_columns):
    counts = processed_data[col].value_counts().sort_index()
    axes[i].bar(counts.index, counts.values)
    axes[i].set_title(f'Distribution of {col}')
    axes[i].set_ylabel('Count')
    axes[i].grid(alpha=0.3)
    

# Also show outcome distribution
counts = processed_data[outcome_column].value_counts().sort_index()
axes[-1].bar(counts.index, counts.values)
axes[-1].set_title(f'Distribution of {outcome_column}')
axes[-1].set_ylabel('Count')
axes[-1].set_xticks([0, 1])
axes[-1].set_xticklabels(['<=50K', '>50K'])
axes[-1].grid(alpha=0.3)

    

plt.tight_layout()
plt.show()






# Create a figure to show relationship between sensitive attributes and outcome
fig, axes = plt.subplots(1, len(sensitive_columns), figsize=(15, 5))

for i, col in enumerate(sensitive_columns):
    # Calculate outcome rate for each value of the sensitive attribute
    group_outcomes = processed_data.groupby(col)[outcome_column].mean()
    
    axes[i].bar(group_outcomes.index, group_outcomes.values)
    axes[i].set_title(f'Outcome Rate by {col}')
    axes[i].set_ylabel('Probability of >50K')
    axes[i].set_ylim(0, 1)
    axes[i].grid(alpha=0.3)

plt.tight_layout()
plt.show()





# Split data into training and testing sets
X = processed_data[nonsensitive_columns]
y = processed_data[outcome_column]
sensitive = processed_data[sensitive_columns]

X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(
    X, y, sensitive, test_size=0.3, random_state=42
)

# Reconstruct DataFrames
train_data = pd.concat([X_train, sensitive_train, y_train], axis=1)
test_data = pd.concat([X_test, sensitive_test, y_test], axis=1)

print(f"Training set: {len(train_data)} samples")
print(f"Testing set: {len(test_data)} samples")





# Make sure we're only using numerical features for clustering
# This extracts only the numerical columns from X_train to avoid the error
numerical_cols = X_train.select_dtypes(include=['number']).columns
X_train_numeric = X_train[numerical_cols]
X_test_numeric = X_test[numerical_cols]

print(f"Using {len(numerical_cols)} numerical features for clustering")

# Initialize the clustering algorithm
clustering = MultiClusteringAlgorithm()

# Try finding optimal number of clusters using the Gap statistic
# Note: This can be time-consuming, so we'll use a subset of the data
sample_size = min(5000, len(X_train_numeric))
X_sample = X_train_numeric.sample(sample_size, random_state=42).values

# Find optimal k (using k-means for speed)
try:
    optimal_k, gap_values = clustering.find_optimal_k(
        X_sample, 
        algorithm='kmeans', 
        k_range=range(2, 6),  # Using a smaller range for demonstration
        random_state=42
    )
    print(f"Optimal number of clusters according to Gap statistic: {optimal_k}")
    
    # Plot the Gap statistic
    clustering.plot_gap_statistic(range(2, 6), gap_values)
except Exception as e:
    print(f"Error in finding optimal k: {str(e)}")
    # Fall back to a reasonable default
    optimal_k = 4
    print(f"Falling back to default k = {optimal_k}")





# Try different clustering algorithms with the optimal k
algorithms = ['kmeans', 'gmm']  # Removed spectral and ensemble which can be slow
cluster_results = {}
cmi_values = {}

for algorithm in algorithms:
    print(f"\nTrying {algorithm} clustering...")
    try:
        # Use the numerical data for clustering
        clusters = clustering.fit(X_train_numeric.values, algorithm=algorithm, n_clusters=optimal_k, random_state=42)
        
        # Evaluate clustering quality
        metrics = clustering.evaluate_clusters(X_train_numeric.values, clusters)
        print(f"Silhouette score: {metrics['silhouette_score']:.3f}")
        print(f"Davies-Bouldin index: {metrics['davies_bouldin_index']:.3f}")
        
        # Calculate CMI
        # We need to add the clusters to train_data
        train_data_with_clusters = train_data.copy()
        train_data_with_clusters['cluster'] = clusters
        
        cmi = calculate_cmi(
            train_data_with_clusters, 
            clusters, 
            sensitive_columns, 
            outcome_column, 
            nonsensitive_columns
        )
        print(f"CMI: {cmi:.4f}")
        
        # Store results
        cluster_results[algorithm] = clusters
        cmi_values[algorithm] = cmi
    except Exception as e:
        print(f"Error with {algorithm}: {e}")





# Find the algorithm with the highest CMI (best at revealing discrimination)
if cmi_values:
    best_algorithm = max(cmi_values, key=cmi_values.get)
    best_clusters = cluster_results[best_algorithm]

    print(f"Best algorithm: {best_algorithm} with CMI = {cmi_values[best_algorithm]:.4f}")

    # Visualize the clusters using PCA
    clustering.plot_clusters(X_train_numeric.values, best_clusters)

    # Also visualize with sensitive attributes coloring
    if 'sex' in sensitive_columns:
        clustering.plot_clusters(
            X_train_numeric.values, 
            best_clusters, 
            sensitive_attr=sensitive_train['sex'].values
        )

    if 'race' in sensitive_columns:
        clustering.plot_clusters(
            X_train_numeric.values, 
            best_clusters, 
            sensitive_attr=sensitive_train['race'].values
        )
else:
    print("No clustering algorithm completed successfully")





# Create a dataset with clusters for further analysis
train_data_with_clusters = train_data.copy()
train_data_with_clusters['cluster'] = best_clusters

# Calculate CMI per cluster
print("Calculating CMI for each cluster...")
cmi_per_cluster = calculate_cmi_per_cluster(
    train_data_with_clusters,
    best_clusters,
    sensitive_columns,
    outcome_column,
    nonsensitive_columns
)

# Plot CMI by cluster
plt.figure(figsize=(10, 6))
clusters = list(cmi_per_cluster.keys())
cmi_values_list = list(cmi_per_cluster.values())

# Sort by CMI value
sorted_indices = np.argsort(cmi_values_list)[::-1]
sorted_clusters = [clusters[i] for i in sorted_indices]
sorted_values = [cmi_values_list[i] for i in sorted_indices]

plt.bar(sorted_clusters, sorted_values)
plt.xlabel('Cluster')
plt.ylabel('CMI')
plt.title('Conditional Mutual Information by Cluster')
plt.xticks(sorted_clusters)
plt.grid(True, alpha=0.3)
plt.show()

# Identify high-discrimination clusters
threshold = np.mean(list(cmi_per_cluster.values())) + 0.5 * np.std(list(cmi_per_cluster.values()))
high_discrim_clusters = [c for c, v in cmi_per_cluster.items() if v > threshold]

print("\nCMI per cluster:")
for cluster_id, cluster_cmi in sorted(cmi_per_cluster.items(), key=lambda x: x[1], reverse=True):
    print(f"  Cluster {cluster_id}: {cluster_cmi:.4f}" + 
          (" (high discrimination)" if cluster_id in high_discrim_clusters else ""))





for cluster_id in high_discrim_clusters:
    print(f"\nCluster {cluster_id} characteristics:")
    cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == cluster_id]
    
    # Outcome rate
    outcome_rate = cluster_data[outcome_column].mean()
    print(f"  Outcome rate: {outcome_rate:.2f}")
    print(f"  Size: {len(cluster_data)} samples")
    
    # Distribution by sensitive attributes
    for col in sensitive_columns:
        print(f"\n  Distribution by {col}:")
        value_counts = cluster_data[col].value_counts(normalize=True)
        
        for value, proportion in value_counts.items():
            # Get outcome rate for this subgroup
            subgroup = cluster_data[cluster_data[col] == value]
            subgroup_outcome_rate = subgroup[outcome_column].mean()
            
            print(f"    {col}={value}: {proportion:.2f} of cluster, outcome rate: {subgroup_outcome_rate:.2f}")





# Pick the cluster with the highest CMI
if high_discrim_clusters:
    target_cluster = high_discrim_clusters[0]
    
    # Extract data for this cluster
    cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == target_cluster]
    cluster_size = len(cluster_data)
    
    print(f"Validating discrimination in cluster {target_cluster} (size: {cluster_size})...")
    
    # Set all samples to same cluster (since we're analyzing within a single cluster)
    cluster_assignments = np.zeros(cluster_size)
    
    # Perform permutation test
    print("\nPerforming permutation test...")
    perm_results = permutation_test(
        cluster_data,
        cluster_assignments,
        sensitive_columns,
        outcome_column,
        nonsensitive_columns,
        num_permutations=100  # Use more (e.g., 1000) for a real analysis
    )
    
    # Plot permutation test results
    plot_permutation_test(perm_results)
    
    # Bootstrap confidence interval
    print("\nCalculating bootstrap confidence interval...")
    bootstrap_results = bootstrap_ci(
        cluster_data,
        cluster_assignments,
        sensitive_columns,
        outcome_column,
        nonsensitive_columns,
        num_bootstraps=100  # Use more (e.g., 1000) for a real analysis
    )
    
    # Plot bootstrap distribution
    plot_bootstrap_distribution(bootstrap_results)
else:
    print("No high-discrimination clusters identified.")





if high_discrim_clusters:
    print("Hierarchical CMI decomposition for high-discrimination clusters:")
    
    for cluster_id in high_discrim_clusters:
        cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == cluster_id]
        
        contributions = hierarchical_cmi_decomposition(
            cluster_data,
            np.zeros(len(cluster_data)),  # All in same cluster
            sensitive_columns,
            outcome_column,
            nonsensitive_columns
        )
        
        print(f"\nCluster {cluster_id} contributions:")
        cluster_cmi = cmi_per_cluster[cluster_id]
        
        # Create a bar chart of contributions
        plt.figure(figsize=(8, 5))
        attrs = list(contributions.keys())
        contrib_values = list(contributions.values())
        percentages = [value/cluster_cmi*100 for value in contrib_values]
        
        plt.bar(attrs, percentages)
        plt.xlabel('Sensitive Attribute')
        plt.ylabel('Contribution (%)')
        plt.title(f'Attribute Contributions to CMI in Cluster {cluster_id}')
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 100)
        
        # Add percentage labels
        for i, p in enumerate(percentages):
            plt.annotate(f'{p:.1f}%', 
                        (i, p), 
                        ha='center', va='bottom')
        
        plt.show()
        
        # Print textual results
        for attr, value in contributions.items():
            print(f"  {attr}: {value:.4f} ({value/cluster_cmi*100:.1f}%)")
        
    # Interaction information (if there are at least 2 sensitive attributes)
    if len(sensitive_columns) >= 2:
        print("\nInteraction information for high-discrimination clusters:")
        
        for cluster_id in high_discrim_clusters:
            cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == cluster_id]
            
            interaction = interaction_information(
                cluster_data,
                np.zeros(len(cluster_data)),  # All in same cluster
                sensitive_columns[0],
                sensitive_columns[1],
                outcome_column,
                nonsensitive_columns
            )
            
            # Create visualization for interaction information
            plt.figure(figsize=(6, 4))
            plt.bar(['Interaction Information'], [interaction])
            plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            
            effect_type = "Synergistic" if interaction > 0 else "Redundant"
            plt.title(f'{effect_type} Effect in Cluster {cluster_id}')
            plt.ylabel('Interaction Information')
            plt.annotate(f'{interaction:.4f}', 
                        (0, interaction), 
                        ha='center', 
                        va='bottom' if interaction > 0 else 'top')
            
            plt.show()
            
            print(f"\nCluster {cluster_id}:")
            print(f"  Interaction information: {interaction:.4f}")
            effect_type = "Synergistic" if interaction > 0 else "Redundant"
            print(f"  {effect_type} effect between {sensitive_columns[0]} and {sensitive_columns[1]}")





print("Applying reweighting strategy...")
reweighted_data = reweighting(train_data, best_clusters, sensitive_columns, outcome_column)

# Visualize weight distribution
plt.figure(figsize=(10, 6))
plt.hist(reweighted_data['weight'], bins=30)
plt.xlabel('Weight')
plt.ylabel('Frequency')
plt.title('Distribution of Weights after Reweighting')
plt.grid(True, alpha=0.3)
plt.show()





# Import scikit-learn metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Train model with fairness regularization
print("Training model with fairness regularization...")
fair_model = FairnessRegularizedModel(lambda_param=1.0)
fair_model.fit(X_train, y_train, sensitive_train, best_clusters, nonsensitive_columns)

# Make predictions on test set
y_pred = fair_model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Test accuracy: {accuracy:.4f}")

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))





# Apply subgroup calibration
print("Applying subgroup-specific calibration...")
# First make sure the outcome column is explicitly identified
outcome_column_name = y_train.name if hasattr(y_train, 'name') else outcome_column

# Make sure train_data has all the necessary columns
train_data_for_calibration = train_data.copy()
if 'cluster' not in train_data_for_calibration.columns:
    train_data_for_calibration['cluster'] = best_clusters

try:
    calibration_results = subgroup_calibration(
        train_data_for_calibration, 
        best_clusters, 
        sensitive_columns,
        # You can optionally provide a base model here
        base_model=None
    )
    
    # Print performances by subgroup
    print("\nCalibration performance by subgroup:")
    
    if 'performance' in calibration_results and calibration_results['performance']:
        performances = calibration_results['performance']
        
        # Sort by improvement
        improvements = [(key, perf['calibrated_acc'] - perf['base_acc']) 
                       for key, perf in performances.items()]
        improvements.sort(key=lambda x: x[1], reverse=True)
        
        for key, improvement in improvements:
            perf = performances[key]
            print(f"  {key}: {perf['base_acc']:.4f} -> {perf['calibrated_acc']:.4f} " + 
                  f"({improvement*100:+.2f}%, {perf['samples']} samples)")
    else:
        print("No performance metrics available. This may be due to insufficient samples in subgroups.")
    
except Exception as e:
    print(f"Error during calibration: {str(e)}")
    print("\nFalling back to simple model comparison without calibration...")
    
    # Train a regular model for comparison
    from sklearn.linear_model import LogisticRegression
    regular_model = LogisticRegression(max_iter=1000)
    regular_model.fit(X_train, y_train)
    
    # Train a weighted model using reweighting
    weighted_data = reweighting(train_data, best_clusters, sensitive_columns, outcome_column_name)
    weighted_model = LogisticRegression(max_iter=1000)
    weighted_model.fit(
        X_train, 
        y_train,
        sample_weight=weighted_data['weight'].values if 'weight' in weighted_data.columns else None
    )
    
    # Compare on test set
    regular_preds = regular_model.predict(X_test)
    weighted_preds = weighted_model.predict(X_test)
    
    print(f"Regular model accuracy: {accuracy_score(y_test, regular_preds):.4f}")
    print(f"Weighted model accuracy: {accuracy_score(y_test, weighted_preds):.4f}")





# Import required scikit-learn components
from sklearn.neighbors import NearestCentroid
from sklearn.linear_model import LogisticRegression

# Create test data with original and fair predictions
test_data_with_clusters = test_data.copy()

# Get clusters for test data (using nearest centroid)
centroid_classifier = NearestCentroid()
centroid_classifier.fit(X_train_numeric.values, best_clusters)
test_clusters = centroid_classifier.predict(X_test_numeric.values)
test_data_with_clusters['cluster'] = test_clusters

# Get predictions from a regular model for comparison
regular_model = LogisticRegression(max_iter=1000)
regular_model.fit(X_train, y_train)
regular_preds = regular_model.predict(X_test)

# Create datasets with predictions
test_regular = test_data_with_clusters.copy()
test_regular['prediction'] = regular_preds

test_fair = test_data_with_clusters.copy()
test_fair['prediction'] = y_pred

# Calculate CMI for both prediction sets
print("Evaluating mitigation effectiveness...")
regular_cmi = calculate_cmi(
    test_regular,
    test_clusters,
    sensitive_columns,
    'prediction',
    nonsensitive_columns
)

fair_cmi = calculate_cmi(
    test_fair,
    test_clusters,
    sensitive_columns,
    'prediction',
    nonsensitive_columns
)

print(f"CMI on regular predictions: {regular_cmi:.4f}")
print(f"CMI on fair predictions: {fair_cmi:.4f}")
print(f"CMI reduction: {(1 - fair_cmi/regular_cmi)*100:.2f}%")

# Calculate accuracy
regular_acc = accuracy_score(y_test, regular_preds)
fair_acc = accuracy_score(y_test, y_pred)

print(f"Regular model accuracy: {regular_acc:.4f}")
print(f"Fair model accuracy: {fair_acc:.4f}")
print(f"Accuracy change: {(fair_acc - regular_acc)*100:+.2f}%")

# Calculate CMI per cluster
regular_cmi_per_cluster = calculate_cmi_per_cluster(
    test_regular,
    test_clusters,
    sensitive_columns,
    'prediction',
    nonsensitive_columns
)

fair_cmi_per_cluster = calculate_cmi_per_cluster(
    test_fair,
    test_clusters,
    sensitive_columns,
    'prediction',
    nonsensitive_columns
)

# Compare CMI reduction across clusters
print("\nCMI reduction by cluster:")
for cluster in sorted(regular_cmi_per_cluster.keys()):
    if cluster in fair_cmi_per_cluster:
        reg_cmi = regular_cmi_per_cluster[cluster]
        fair_cmi = fair_cmi_per_cluster[cluster]
        reduction = (1 - fair_cmi/reg_cmi)*100
        print(f"  Cluster {cluster}: {reg_cmi:.4f} -> {fair_cmi:.4f} ({reduction:.2f}% reduction)")





# Create a summary of our findings
print("Summary of Discrimination Detection Analysis:")
print(f"1. Optimal number of clusters: {optimal_k}")
print(f"2. Best algorithm: {best_algorithm}")
print(f"3. Overall CMI: {cmi_values[best_algorithm]:.4f}")
print(f"4. High discrimination clusters: {high_discrim_clusters}")

if high_discrim_clusters:
    # Get highest contributing attribute for each high-discrimination cluster
    for cluster_id in high_discrim_clusters:
        cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == cluster_id]
        
        contributions = hierarchical_cmi_decomposition(
            cluster_data,
            np.zeros(len(cluster_data)),
            sensitive_columns,
            outcome_column,
            nonsensitive_columns
        )
        
        main_contributor = max(contributions.items(), key=lambda x: x[1])[0]
        print(f"5. Cluster {cluster_id} main contributor: {main_contributor}")
        
        if len(sensitive_columns) >= 2:
            interaction = interaction_information(
                cluster_data,
                np.zeros(len(cluster_data)),
                sensitive_columns[0],
                sensitive_columns[1],
                outcome_column,
                nonsensitive_columns
            )
            
            effect_type = "Synergistic" if interaction > 0 else "Redundant"
            print(f"6. {effect_type} effect between {sensitive_columns[0]} and {sensitive_columns[1]}")

print(f"7. Mitigation: {(1 - fair_cmi/regular_cmi)*100:.2f}% CMI reduction with {(fair_acc - regular_acc)*100:+.2f}% accuracy change")



























