{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Hidden Discrimination in the Adult Dataset\n",
    "This notebook demonstrates how to use the discrimination detection framework to identify and mitigate hidden discrimination in the Adult dataset.\n",
    "# Setup\n",
    "First, let's import all the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add the parent directory to the path to import our modules\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our modules\n",
    "from src.preprocessing import load_adult_dataset, preprocess_adult_dataset\n",
    "from src.clustering import MultiClusteringAlgorithm\n",
    "from src.cmi import calculate_cmi, calculate_cmi_per_cluster, hierarchical_cmi_decomposition, interaction_information\n",
    "from src.validation import permutation_test, bootstrap_ci, plot_permutation_test, plot_bootstrap_distribution\n",
    "from src.mitigation import reweighting, FairnessRegularizedModel, subgroup_calibration, evaluate_mitigation, plot_mitigation_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and Preprocess Data\n",
    "Let's load the Adult dataset and preprocess it as described in the paper:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Adult dataset if not already available\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "data_dir = '../data'\n",
    "adult_data_path = os.path.join(data_dir, 'adult.data')\n",
    "\n",
    "if not os.path.exists(adult_data_path):\n",
    "    print(\"Downloading Adult dataset...\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
    "        adult_data_path\n",
    "    )\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# Load the dataset\n",
    "adult_data = load_adult_dataset(adult_data_path)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset shape: {adult_data.shape}\")\n",
    "print(adult_data.head())\n",
    "\n",
    "# Preprocess the dataset\n",
    "processed_data, sensitive_columns, nonsensitive_columns, outcome_column = preprocess_adult_dataset(adult_data)\n",
    "\n",
    "print(f\"Processed dataset shape: {processed_data.shape}\")\n",
    "print(f\"Sensitive columns: {sensitive_columns}\")\n",
    "print(f\"Outcome column: {outcome_column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the distribution of the sensitive attributes and outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at distributions of sensitive attributes\n",
    "fig, axes = plt.subplots(1, len(sensitive_columns) + 1, figsize=(15, 5))\n",
    "\n",
    "for i, col in enumerate(sensitive_columns):\n",
    "    counts = processed_data[col].value_counts().sort_index()\n",
    "    axes[i].bar(counts.index, counts.values)\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].grid(alpha=0.3)\n",
    "    \n",
    "\n",
    "# Also show outcome distribution\n",
    "counts = processed_data[outcome_column].value_counts().sort_index()\n",
    "axes[-1].bar(counts.index, counts.values)\n",
    "axes[-1].set_title(f'Distribution of {outcome_column}')\n",
    "axes[-1].set_ylabel('Count')\n",
    "axes[-1].set_xticks([0, 1])\n",
    "axes[-1].set_xticklabels(['<=50K', '>50K'])\n",
    "axes[-1].grid(alpha=0.3)\n",
    "\n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the relationship between sensitive attributes and outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure to show relationship between sensitive attributes and outcome\n",
    "fig, axes = plt.subplots(1, len(sensitive_columns), figsize=(15, 5))\n",
    "\n",
    "for i, col in enumerate(sensitive_columns):\n",
    "    # Calculate outcome rate for each value of the sensitive attribute\n",
    "    group_outcomes = processed_data.groupby(col)[outcome_column].mean()\n",
    "    \n",
    "    axes[i].bar(group_outcomes.index, group_outcomes.values)\n",
    "    axes[i].set_title(f'Outcome Rate by {col}')\n",
    "    axes[i].set_ylabel('Probability of >50K')\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split Data and Perform Clustering\n",
    "Let's split the data into training and testing sets, then perform clustering on the non-sensitive attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = processed_data[nonsensitive_columns]\n",
    "y = processed_data[outcome_column]\n",
    "sensitive = processed_data[sensitive_columns]\n",
    "\n",
    "X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(\n",
    "    X, y, sensitive, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Reconstruct DataFrames\n",
    "train_data = pd.concat([X_train, sensitive_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, sensitive_test, y_test], axis=1)\n",
    "\n",
    "print(f\"Training set: {len(train_data)} samples\")\n",
    "print(f\"Testing set: {len(test_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try different clustering algorithms and find the optimal number of clusters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the clustering algorithm\n",
    "clustering = MultiClusteringAlgorithm()\n",
    "\n",
    "# Try finding optimal number of clusters using the Gap statistic\n",
    "# Note: This can be time-consuming, so we'll use a subset of the data\n",
    "sample_size = min(10000, len(X_train))\n",
    "X_sample = X_train.sample(sample_size, random_state=42).values\n",
    "\n",
    "# Find optimal k (using k-means for speed)\n",
    "optimal_k, gap_values = clustering.find_optimal_k(\n",
    "    X_sample, \n",
    "    algorithm='kmeans', \n",
    "    k_range=range(2, 11),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Optimal number of clusters according to Gap statistic: {optimal_k}\")\n",
    "\n",
    "# Plot the Gap statistic\n",
    "clustering.plot_gap_statistic(range(2, 11), gap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the optimal number of clusters, let's compare different clustering algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different clustering algorithms with the optimal k\n",
    "algorithms = ['kmeans', 'gmm', 'spectral', 'ensemble']\n",
    "cluster_results = {}\n",
    "cmi_values = {}\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    print(f\"\\nTrying {algorithm} clustering...\")\n",
    "    try:\n",
    "        if algorithm == 'ensemble':\n",
    "            clusters = clustering.ensemble_clustering(X_train.values, n_clusters=optimal_k, random_state=42)\n",
    "        else:\n",
    "            clusters = clustering.fit(X_train.values, algorithm=algorithm, n_clusters=optimal_k, random_state=42)\n",
    "        \n",
    "        # Evaluate clustering quality\n",
    "        metrics = clustering.evaluate_clusters(X_train.values, clusters)\n",
    "        print(f\"Silhouette score: {metrics['silhouette_score']:.3f}\")\n",
    "        print(f\"Davies-Bouldin index: {metrics['davies_bouldin_index']:.3f}\")\n",
    "        \n",
    "        # Calculate CMI\n",
    "        cmi = calculate_cmi(\n",
    "            train_data, \n",
    "            clusters, \n",
    "            sensitive_columns, \n",
    "            outcome_column, \n",
    "            nonsensitive_columns\n",
    "        )\n",
    "        print(f\"CMI: {cmi:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        cluster_results[algorithm] = clusters\n",
    "        cmi_values[algorithm] = cmi\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {algorithm}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the clusters from the best algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the algorithm with the highest CMI (best at revealing discrimination)\n",
    "best_algorithm = max(cmi_values, key=cmi_values.get)\n",
    "best_clusters = cluster_results[best_algorithm]\n",
    "\n",
    "print(f\"Best algorithm: {best_algorithm} with CMI = {cmi_values[best_algorithm]:.4f}\")\n",
    "\n",
    "# Visualize the clusters using PCA\n",
    "clustering.plot_clusters(X_train.values, best_clusters)\n",
    "\n",
    "# Also visualize with sensitive attributes coloring\n",
    "if 'sex' in sensitive_columns:\n",
    "    clustering.plot_clusters(\n",
    "        X_train.values, \n",
    "        best_clusters, \n",
    "        sensitive_attr=sensitive_train['sex'].values\n",
    "    )\n",
    "\n",
    "if 'race' in sensitive_columns:\n",
    "    clustering.plot_clusters(\n",
    "        X_train.values, \n",
    "        best_clusters, \n",
    "        sensitive_attr=sensitive_train['race'].values\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculate CMI and Identify High-Discrimination Clusters\n",
    "Now, let's calculate the CMI for each cluster to identify ones with high discrimination:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CMI per cluster\n",
    "print(\"Calculating CMI for each cluster...\")\n",
    "cmi_per_cluster = calculate_cmi_per_cluster(\n",
    "    train_data,\n",
    "    best_clusters,\n",
    "    sensitive_columns,\n",
    "    outcome_column,\n",
    "    nonsensitive_columns\n",
    ")\n",
    "\n",
    "# Add clusters to training data for further analysis\n",
    "train_data_with_clusters = train_data.copy()\n",
    "train_data_with_clusters['cluster'] = best_clusters\n",
    "\n",
    "# Plot CMI by cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "clusters = list(cmi_per_cluster.keys())\n",
    "cmi_values_list = list(cmi_per_cluster.values())\n",
    "\n",
    "# Sort by CMI value\n",
    "sorted_indices = np.argsort(cmi_values_list)[::-1]\n",
    "sorted_clusters = [clusters[i] for i in sorted_indices]\n",
    "sorted_values = [cmi_values_list[i] for i in sorted_indices]\n",
    "\n",
    "plt.bar(sorted_clusters, sorted_values)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('CMI')\n",
    "plt.title('Conditional Mutual Information by Cluster')\n",
    "plt.xticks(sorted_clusters)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Identify high-discrimination clusters\n",
    "threshold = np.mean(list(cmi_per_cluster.values())) + 0.5 * np.std(list(cmi_per_cluster.values()))\n",
    "high_discrim_clusters = [c for c, v in cmi_per_cluster.items() if v > threshold]\n",
    "\n",
    "print(\"\\nCMI per cluster:\")\n",
    "for cluster_id, cluster_cmi in sorted(cmi_per_cluster.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  Cluster {cluster_id}: {cluster_cmi:.4f}\" + \n",
    "          (\" (high discrimination)\" if cluster_id in high_discrim_clusters else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CMI by cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "clusters = list(cmi_per_cluster.keys())\n",
    "cmi_values_list = list(cmi_per_cluster.values())\n",
    "\n",
    "# Sort by CMI value\n",
    "sorted_indices = np.argsort(cmi_values_list)[::-1]\n",
    "sorted_clusters = [clusters[i] for i in sorted_indices]\n",
    "sorted_values = [cmi_values_list[i] for i in sorted_indices]\n",
    "\n",
    "# Define discrimination thresholds\n",
    "high_threshold = 0.1\n",
    "medium_threshold = 0.05\n",
    "\n",
    "# Color bars based on discrimination level\n",
    "colors = ['red' if val > high_threshold else 'orange' if val > medium_threshold else 'blue' for val in sorted_values]\n",
    "\n",
    "# Create bar chart with colors\n",
    "bars = plt.bar(sorted_clusters, sorted_values, color=colors)\n",
    "\n",
    "# Add threshold lines\n",
    "plt.axhline(y=high_threshold, color='red', linestyle='--', label='High discrimination (0.1)')\n",
    "plt.axhline(y=medium_threshold, color='orange', linestyle='--', label='Medium discrimination (0.05)')\n",
    "\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('CMI')\n",
    "plt.title('Discrimination Level by Cluster')\n",
    "plt.xticks(sorted_clusters)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('discrimination_by_cluster.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Identify high-discrimination clusters\n",
    "print(\"\\nCMI per cluster:\")\n",
    "for cluster_id, cluster_cmi in sorted(cmi_per_cluster.items(), key=lambda x: x[1], reverse=True):\n",
    "    discrimination_level = \"high discrimination\" if cluster_cmi > high_threshold else \\\n",
    "                          \"medium discrimination\" if cluster_cmi > medium_threshold else \\\n",
    "                          \"low discrimination\"\n",
    "    print(f\"  Cluster {cluster_id}: {cluster_cmi:.4f} ({discrimination_level})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the characteristics of high-discrimination clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in high_discrim_clusters:\n",
    "    print(f\"\\nCluster {cluster_id} characteristics:\")\n",
    "    cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == cluster_id]\n",
    "    \n",
    "    # Outcome rate\n",
    "    outcome_rate = cluster_data[outcome_column].mean()\n",
    "    print(f\"  Outcome rate: {outcome_rate:.2f}\")\n",
    "    print(f\"  Size: {len(cluster_data)} samples\")\n",
    "    \n",
    "    # Distribution by sensitive attributes\n",
    "    for col in sensitive_columns:\n",
    "        print(f\"\\n  Distribution by {col}:\")\n",
    "        value_counts = cluster_data[col].value_counts(normalize=True)\n",
    "        \n",
    "        for value, proportion in value_counts.items():\n",
    "            # Get outcome rate for this subgroup\n",
    "            subgroup = cluster_data[cluster_data[col] == value]\n",
    "            subgroup_outcome_rate = subgroup[outcome_column].mean()\n",
    "            \n",
    "            print(f\"    {col}={value}: {proportion:.2f} of cluster, outcome rate: {subgroup_outcome_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in high_discrim_clusters:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"CLUSTER {cluster_id} DETAILED ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get data for this cluster\n",
    "    cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == cluster_id]\n",
    "    \n",
    "    # Basic cluster statistics\n",
    "    outcome_rate = cluster_data[outcome_column].mean()\n",
    "    cluster_size = len(cluster_data)\n",
    "    cluster_percentage = (cluster_size / len(train_data_with_clusters)) * 100\n",
    "    \n",
    "    print(f\"Size: {cluster_size} samples ({cluster_percentage:.1f}% of total data)\")\n",
    "    print(f\"Overall outcome rate: {outcome_rate:.2f}\")\n",
    "    \n",
    "    # CMI value for this cluster\n",
    "    cluster_cmi = cmi_per_cluster.get(cluster_id, \"Not calculated\")\n",
    "    print(f\"Discrimination level (CMI): {cluster_cmi:.4f}\")\n",
    "    \n",
    "    # Analyze individual sensitive attributes\n",
    "    print(\"\\nSENSITIVE ATTRIBUTE ANALYSIS:\")\n",
    "    for col in sensitive_columns:\n",
    "        print(f\"\\n  Distribution by {col}:\")\n",
    "        value_counts = cluster_data[col].value_counts()\n",
    "        total = value_counts.sum()\n",
    "        \n",
    "        # Calculate divergence from overall outcome rate\n",
    "        max_divergence = 0\n",
    "        max_divergence_group = None\n",
    "        \n",
    "        # Create a formatted table for output\n",
    "        print(f\"    {'Value':<15} {'Count':<8} {'Proportion':<12} {'Outcome Rate':<15} {'Divergence':<10}\")\n",
    "        print(f\"    {'-'*60}\")\n",
    "        \n",
    "        for value, count in value_counts.items():\n",
    "            proportion = count / total\n",
    "            subgroup = cluster_data[cluster_data[col] == value]\n",
    "            subgroup_outcome_rate = subgroup[outcome_column].mean()\n",
    "            divergence = abs(subgroup_outcome_rate - outcome_rate)\n",
    "            \n",
    "            # Track maximum divergence\n",
    "            if divergence > max_divergence:\n",
    "                max_divergence = divergence\n",
    "                max_divergence_group = value\n",
    "                \n",
    "            # Format and print the row\n",
    "            print(f\"    {str(value):<15} {count:<8} {proportion:.2f} ({proportion*100:.1f}%) {subgroup_outcome_rate:.2f} {divergence:+.2f}\")\n",
    "        \n",
    "        print(f\"\\n  â†’ Highest divergence in {col}: {max_divergence_group} ({max_divergence:+.2f})\")\n",
    "    \n",
    "    # Intersectional analysis\n",
    "    if len(sensitive_columns) > 1:\n",
    "        print(\"\\nINTERSECTIONAL ANALYSIS:\")\n",
    "        # Create intersection column\n",
    "        cluster_data['intersection'] = cluster_data[sensitive_columns].apply(\n",
    "            lambda row: '_'.join(str(row[col]) for col in sensitive_columns), axis=1\n",
    "        )\n",
    "        \n",
    "        # Get intersectional counts and rates\n",
    "        intersection_counts = cluster_data['intersection'].value_counts()\n",
    "        \n",
    "        # Only show intersections with at least 20 samples (for statistical reliability)\n",
    "        threshold = 20\n",
    "        valid_intersections = intersection_counts[intersection_counts >= threshold]\n",
    "        \n",
    "        if len(valid_intersections) > 0:\n",
    "            print(f\"\\n  Showing intersections with at least {threshold} samples:\")\n",
    "            print(f\"    {'Intersection':<25} {'Count':<8} {'Proportion':<12} {'Outcome Rate':<15} {'Divergence':<10}\")\n",
    "            print(f\"    {'-'*70}\")\n",
    "            \n",
    "            for intersection, count in valid_intersections.items():\n",
    "                proportion = count / total\n",
    "                subgroup = cluster_data[cluster_data['intersection'] == intersection]\n",
    "                subgroup_outcome_rate = subgroup[outcome_column].mean()\n",
    "                divergence = subgroup_outcome_rate - outcome_rate\n",
    "                \n",
    "                print(f\"    {intersection:<25} {count:<8} {proportion:.2f} ({proportion*100:.1f}%) {subgroup_outcome_rate:.2f} {divergence:+.2f}\")\n",
    "        else:\n",
    "            print(f\"  No intersections with at least {threshold} samples found.\")\n",
    "    \n",
    "    # Feature importance for this cluster\n",
    "    print(\"\\nKEY FEATURES CHARACTERIZING THIS CLUSTER:\")\n",
    "    # Get all non-sensitive columns except the outcome and cluster columns\n",
    "    feature_columns = [col for col in cluster_data.columns \n",
    "                      if col not in sensitive_columns + [outcome_column, 'cluster', 'intersection']]\n",
    "    \n",
    "    # Calculate the mean value of each feature in this cluster vs. overall\n",
    "    feature_comparison = []\n",
    "    for col in feature_columns:\n",
    "        try:\n",
    "            # Only analyze numerical columns\n",
    "            if pd.api.types.is_numeric_dtype(cluster_data[col]):\n",
    "                cluster_mean = cluster_data[col].mean()\n",
    "                overall_mean = train_data_with_clusters[col].mean()\n",
    "                std_dev = train_data_with_clusters[col].std()\n",
    "                \n",
    "                # Calculate z-score to measure how distinctive this feature is\n",
    "                if std_dev > 0:\n",
    "                    z_score = (cluster_mean - overall_mean) / std_dev\n",
    "                    feature_comparison.append((col, cluster_mean, overall_mean, z_score))\n",
    "        except:\n",
    "            # Skip columns that cause errors\n",
    "            continue\n",
    "    \n",
    "    # Sort by absolute z-score and display top features\n",
    "    top_n = min(10, len(feature_comparison))\n",
    "    feature_comparison.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "    \n",
    "    if feature_comparison:\n",
    "        print(f\"  Top {top_n} distinctive features:\")\n",
    "        print(f\"    {'Feature':<20} {'Cluster Mean':<15} {'Overall Mean':<15} {'Z-Score':<10}\")\n",
    "        print(f\"    {'-'*60}\")\n",
    "        \n",
    "        for i in range(top_n):\n",
    "            col, c_mean, o_mean, z = feature_comparison[i]\n",
    "            print(f\"    {col:<20} {c_mean:.2f} {o_mean:.2f} {z:+.2f}\")\n",
    "    else:\n",
    "        print(\"  No numerical features available for analysis.\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the visualization style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "\n",
    "# Create a figure for comparison of all high discrimination clusters\n",
    "fig_overview = plt.figure(figsize=(10, 6))\n",
    "ax_overview = fig_overview.add_subplot(111)\n",
    "\n",
    "# Prepare data for overview comparison\n",
    "cluster_summary = []\n",
    "\n",
    "# Loop through each high discrimination cluster\n",
    "for cluster_id in high_discrim_clusters:\n",
    "    cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == cluster_id]\n",
    "    \n",
    "    # Outcome rate\n",
    "    outcome_rate = cluster_data[outcome_column].mean()\n",
    "    print(f\"\\nCluster {cluster_id} characteristics:\")\n",
    "    print(f\"  Outcome rate: {outcome_rate:.2f}\")\n",
    "    print(f\"  Size: {len(cluster_data)} samples\")\n",
    "    \n",
    "    # Store summary data for overview plot\n",
    "    cluster_summary.append({\n",
    "        'cluster_id': cluster_id,\n",
    "        'size': len(cluster_data),\n",
    "        'outcome_rate': outcome_rate\n",
    "    })\n",
    "    \n",
    "    # Create a figure for this cluster's detailed analysis\n",
    "    fig, axes = plt.subplots(len(sensitive_columns), 2, figsize=(15, 4 * len(sensitive_columns)))\n",
    "    fig.suptitle(f'Cluster {cluster_id} Characteristics (Size: {len(cluster_data)} samples, Overall outcome rate: {outcome_rate:.2f})', \n",
    "                fontsize=16)\n",
    "    \n",
    "    # Analyze each sensitive attribute\n",
    "    for idx, col in enumerate(sensitive_columns):\n",
    "        print(f\"\\n  Distribution by {col}:\")\n",
    "        \n",
    "        # Get value counts and prepare visualization data\n",
    "        value_counts = cluster_data[col].value_counts(normalize=True)\n",
    "        subgroup_stats = []\n",
    "        \n",
    "        for value, proportion in value_counts.items():\n",
    "            # Get outcome rate for this subgroup\n",
    "            subgroup = cluster_data[cluster_data[col] == value]\n",
    "            subgroup_outcome_rate = subgroup[outcome_column].mean()\n",
    "            \n",
    "            print(f\"    {col}={value}: {proportion:.2f} of cluster, outcome rate: {subgroup_outcome_rate:.2f}\")\n",
    "            \n",
    "            # Store for plotting\n",
    "            subgroup_stats.append({\n",
    "                'value': str(value),\n",
    "                'proportion': proportion,\n",
    "                'outcome_rate': subgroup_outcome_rate,\n",
    "                'size': len(subgroup)\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame for easier plotting\n",
    "        subgroup_df = pd.DataFrame(subgroup_stats)\n",
    "        \n",
    "        # Left plot: Distribution within cluster by sensitive attribute\n",
    "        if len(sensitive_columns) == 1:\n",
    "            ax1 = axes[0]\n",
    "            ax2 = axes[1]\n",
    "        else:\n",
    "            ax1 = axes[idx, 0]\n",
    "            ax2 = axes[idx, 1]\n",
    "            \n",
    "        # Create horizontal bar chart for distribution\n",
    "        bars = ax1.barh(subgroup_df['value'], subgroup_df['proportion'], color='skyblue')\n",
    "        ax1.set_title(f'Distribution by {col}')\n",
    "        ax1.set_xlabel('Proportion within cluster')\n",
    "        ax1.set_xlim(0, 1)\n",
    "        \n",
    "        # Add proportion labels\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            ax1.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{width:.2f}', va='center')\n",
    "        \n",
    "        # Right plot: Outcome rates by subgroup\n",
    "        bars = ax2.barh(subgroup_df['value'], subgroup_df['outcome_rate'], color='lightcoral')\n",
    "        ax2.set_title(f'Outcome Rates by {col}')\n",
    "        ax2.set_xlabel(f'{outcome_column} Rate')\n",
    "        ax2.set_xlim(0, 1)\n",
    "        \n",
    "        # Add outcome rate labels and sample size\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{width:.2f} (n={subgroup_df.iloc[i][\"size\"]})', va='center')\n",
    "        \n",
    "        # Add a reference line for the overall cluster outcome rate\n",
    "        ax2.axvline(x=outcome_rate, color='red', linestyle='--', \n",
    "                   label=f'Cluster average: {outcome_rate:.2f}')\n",
    "        ax2.legend()\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to make room for the title\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f'cluster_{cluster_id}_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create overview plot comparing all high discrimination clusters\n",
    "cluster_df = pd.DataFrame(cluster_summary)\n",
    "\n",
    "# Sort by outcome rate for better visualization\n",
    "cluster_df = cluster_df.sort_values('outcome_rate', ascending=False)\n",
    "\n",
    "# Plot comparisons\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('High Discrimination Clusters Comparison', fontsize=16)\n",
    "\n",
    "# Left: Cluster sizes\n",
    "size_bars = ax1.bar([f'Cluster {cid}' for cid in cluster_df['cluster_id']], \n",
    "                    cluster_df['size'], color='lightblue')\n",
    "ax1.set_title('Cluster Sizes')\n",
    "ax1.set_xlabel('Cluster')\n",
    "ax1.set_ylabel('Number of Samples')\n",
    "\n",
    "# Add size labels\n",
    "for bar in size_bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, height + 5, \n",
    "            f'{int(height)}', ha='center')\n",
    "\n",
    "# Right: Outcome rates\n",
    "rate_bars = ax2.bar([f'Cluster {cid}' for cid in cluster_df['cluster_id']], \n",
    "                   cluster_df['outcome_rate'], color='salmon')\n",
    "ax2.set_title(f'{outcome_column} Rates')\n",
    "ax2.set_xlabel('Cluster')\n",
    "ax2.set_ylabel('Rate')\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Add rate labels\n",
    "for bar in rate_bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, height + 0.01, \n",
    "            f'{height:.2f}', ha='center')\n",
    "\n",
    "# Overall dataset average for reference\n",
    "overall_rate = train_data_with_clusters[outcome_column].mean()\n",
    "ax2.axhline(y=overall_rate, color='red', linestyle='--', \n",
    "           label=f'Dataset average: {overall_rate:.2f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Save the overview comparison figure\n",
    "plt.savefig('high_discrimination_clusters_comparison.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Statistical Validation\n",
    "Let's validate our findings for one of the high-discrimination clusters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the cluster with the highest CMI\n",
    "if high_discrim_clusters:\n",
    "    target_cluster = high_discrim_clusters[0]\n",
    "    \n",
    "    # Extract data for this cluster\n",
    "    cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == target_cluster]\n",
    "    cluster_size = len(cluster_data)\n",
    "    \n",
    "    print(f\"Validating discrimination in cluster {target_cluster} (size: {cluster_size})...\")\n",
    "    \n",
    "    # Set all samples to same cluster (since we're analyzing within a single cluster)\n",
    "    cluster_assignments = np.zeros(cluster_size)\n",
    "    \n",
    "    # Perform permutation test\n",
    "    print(\"\\nPerforming permutation test...\")\n",
    "    perm_results = permutation_test(\n",
    "        cluster_data,\n",
    "        cluster_assignments,\n",
    "        sensitive_columns,\n",
    "        outcome_column,\n",
    "        nonsensitive_columns,\n",
    "        num_permutations=100  # Use more (e.g., 1000) for a real analysis\n",
    "    )\n",
    "    \n",
    "    # Plot permutation test results\n",
    "    plot_permutation_test(perm_results)\n",
    "   \n",
    "  \n",
    "\n",
    "    \n",
    "    # Bootstrap confidence interval\n",
    "    print(\"\\nCalculating bootstrap confidence interval...\")\n",
    "    bootstrap_results = bootstrap_ci(\n",
    "        cluster_data,\n",
    "        cluster_assignments,\n",
    "        sensitive_columns,\n",
    "        outcome_column,\n",
    "        nonsensitive_columns,\n",
    "        num_bootstraps=1000  # Use more (e.g., 1000) for a real analysis\n",
    "    )\n",
    "    # Save bootstrap CI plot\n",
    "    plot_bootstrap_distribution(bootstrap_results)\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(f'cluster_{target_cluster}_bootstrap_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    \n",
    "\n",
    "    # Plot bootstrap distribution\n",
    "    plot_bootstrap_distribution(bootstrap_results)\n",
    "else:\n",
    "    print(\"No high-discrimination clusters identified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the cluster with the highest CMI\n",
    "if high_discrim_clusters:\n",
    "    target_cluster = high_discrim_clusters[0]\n",
    "    \n",
    "    # Extract data for this cluster\n",
    "    cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == target_cluster]\n",
    "    cluster_size = len(cluster_data)\n",
    "    \n",
    "    print(f\"Validating discrimination in cluster {target_cluster} (size: {cluster_size})...\")\n",
    "    \n",
    "    # Set all samples to same cluster (since we're analyzing within a single cluster)\n",
    "    cluster_assignments = np.zeros(cluster_size)\n",
    "    \n",
    "    # Perform permutation test\n",
    "    print(\"\\nPerforming permutation test...\")\n",
    "    perm_results = permutation_test(\n",
    "        cluster_data,\n",
    "        cluster_assignments,\n",
    "        sensitive_columns,\n",
    "        outcome_column,\n",
    "        nonsensitive_columns,\n",
    "        num_permutations=100  # Use more (e.g., 1000) for a real analysis\n",
    "    )\n",
    "    \n",
    "    # Plot permutation test results and save\n",
    "    # Assuming plot_permutation_test doesn't return a figure but creates one\n",
    "    plot_permutation_test(perm_results)\n",
    "    plt.savefig(f'cluster_{target_cluster}_permutation_test.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Bootstrap confidence interval\n",
    "    print(\"\\nCalculating bootstrap confidence interval...\")\n",
    "    bootstrap_results = bootstrap_ci(\n",
    "        cluster_data,\n",
    "        cluster_assignments,\n",
    "        sensitive_columns,\n",
    "        outcome_column,\n",
    "        nonsensitive_columns,\n",
    "        num_bootstraps=1000  # Use more (e.g., 1000) for a real analysis\n",
    "    )\n",
    "    \n",
    "    # Plot bootstrap distribution and save\n",
    "    # Assuming plot_bootstrap_distribution doesn't return a figure but creates one\n",
    "    plot_bootstrap_distribution(bootstrap_results)\n",
    "    plt.savefig(f'cluster_{target_cluster}_bootstrap_ci.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create and save a combined visualization showing key metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Extract p-value and observed CMI from permutation results\n",
    "    p_value = perm_results['p_value']\n",
    "    observed_cmi = perm_results['observed_cmi']\n",
    "    \n",
    "    # Handle different possible formats of bootstrap_results\n",
    "    # Check the structure of bootstrap_results\n",
    "    if isinstance(bootstrap_results, dict) and 'ci' in bootstrap_results:\n",
    "        lower_ci, upper_ci = bootstrap_results['ci']\n",
    "    elif isinstance(bootstrap_results, dict) and 'lower_ci' in bootstrap_results and 'upper_ci' in bootstrap_results:\n",
    "        lower_ci = bootstrap_results['lower_ci']\n",
    "        upper_ci = bootstrap_results['upper_ci']\n",
    "    elif isinstance(bootstrap_results, tuple) and len(bootstrap_results) == 2:\n",
    "        lower_ci, upper_ci = bootstrap_results\n",
    "    else:\n",
    "        # If we can't determine the CI, use observed value with a default margin\n",
    "        print(\"Warning: Could not extract CI from bootstrap results. Using default CI.\")\n",
    "        lower_ci = observed_cmi * 0.9\n",
    "        upper_ci = observed_cmi * 1.1\n",
    "    \n",
    "    # Create summary visualization\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.bar(['Observed CMI'], [observed_cmi], color='blue')\n",
    "    plt.title(f'Observed CMI: {observed_cmi:.4f}')\n",
    "    plt.ylabel('CMI Value')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(['P-value'], [p_value], color='red' if p_value < 0.05 else 'green')\n",
    "    plt.title(f'Permutation Test P-value: {p_value:.4f}')\n",
    "    plt.axhline(y=0.05, color='r', linestyle='--', label='Significance threshold')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.errorbar(['CMI 95% CI'], [observed_cmi], yerr=[[observed_cmi-lower_ci], [upper_ci-observed_cmi]], \n",
    "                fmt='o', capsize=10, color='purple')\n",
    "    plt.title(f'Bootstrap 95% CI: [{lower_ci:.4f}, {upper_ci:.4f}]')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    # Show cluster info\n",
    "    cluster_info = (f\"Cluster {target_cluster}\\n\"\n",
    "                   f\"Size: {cluster_size} samples\\n\"\n",
    "                   f\"Outcome rate: {cluster_data[outcome_column].mean():.2f}\\n\"\n",
    "                   f\"Significant: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "    plt.text(0.5, 0.5, cluster_info, ha='center', va='center', fontsize=12)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Discrimination Validation for Cluster {target_cluster}', fontsize=16)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    \n",
    "    # Save the summary figure\n",
    "    plt.savefig(f'cluster_{target_cluster}_validation_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No high-discrimination clusters identified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hierarchical Decomposition and Interaction Analysis\n",
    "Let's analyze which sensitive attributes contribute most to discrimination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if high_discrim_clusters:\n",
    "    print(\"Hierarchical CMI decomposition for high-discrimination clusters:\")\n",
    "    \n",
    "    for cluster_id in high_discrim_clusters:\n",
    "        cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == cluster_id]\n",
    "        \n",
    "        contributions = hierarchical_cmi_decomposition(\n",
    "            cluster_data,\n",
    "            np.zeros(len(cluster_data)),  # All in same cluster\n",
    "            sensitive_columns,\n",
    "            outcome_column,\n",
    "            nonsensitive_columns\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nCluster {cluster_id} contributions:\")\n",
    "        cluster_cmi = cmi_per_cluster[cluster_id]\n",
    "        \n",
    "        # Create a bar chart of contributions\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        attrs = list(contributions.keys())\n",
    "        contrib_values = list(contributions.values())\n",
    "        percentages = [value/cluster_cmi*100 for value in contrib_values]\n",
    "        \n",
    "        plt.bar(attrs, percentages)\n",
    "        plt.xlabel('Sensitive Attribute')\n",
    "        plt.ylabel('Contribution (%)')\n",
    "        plt.title(f'Attribute Contributions to CMI in Cluster {cluster_id}')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.ylim(0, 100)\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for i, p in enumerate(percentages):\n",
    "            plt.annotate(f'{p:.1f}%', \n",
    "                        (i, p), \n",
    "                        ha='center', va='bottom')\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(f'cluster_{cluster_id}_contributions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()  # Keep the show command if you still want to display it\n",
    "        \n",
    "        # Print textual results\n",
    "        for attr, value in contributions.items():\n",
    "            print(f\"  {attr}: {value:.4f} ({value/cluster_cmi*100:.1f}%)\")\n",
    "        \n",
    "    # Interaction information (if there are at least 2 sensitive attributes)\n",
    "    if len(sensitive_columns) >= 2:\n",
    "        print(\"\\nInteraction information for high-discrimination clusters:\")\n",
    "        \n",
    "        for cluster_id in high_discrim_clusters:\n",
    "            cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == cluster_id]\n",
    "            \n",
    "            interaction = interaction_information(\n",
    "                cluster_data,\n",
    "                np.zeros(len(cluster_data)),  # All in same cluster\n",
    "                sensitive_columns[0],\n",
    "                sensitive_columns[1],\n",
    "                outcome_column,\n",
    "                nonsensitive_columns\n",
    "            )\n",
    "            \n",
    "            # Create visualization for interaction information\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.bar(['Interaction Information'], [interaction])\n",
    "            plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "            \n",
    "            effect_type = \"Synergistic\" if interaction > 0 else \"Redundant\"\n",
    "            plt.title(f'{effect_type} Effect in Cluster {cluster_id}')\n",
    "            plt.ylabel('Interaction Information')\n",
    "            plt.annotate(f'{interaction:.4f}', \n",
    "                        (0, interaction), \n",
    "                        ha='center', \n",
    "                        va='bottom' if interaction > 0 else 'top')\n",
    "            \n",
    "            # Save the interaction visualization\n",
    "            plt.savefig(f'cluster_{cluster_id}_interaction.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()  # Keep the show command if you still want to display it\n",
    "            \n",
    "            print(f\"\\nCluster {cluster_id}:\")\n",
    "            print(f\"  Interaction information: {interaction:.4f}\")\n",
    "            effect_type = \"Synergistic\" if interaction > 0 else \"Redundant\"\n",
    "            print(f\"  {effect_type} effect between {sensitive_columns[0]} and {sensitive_columns[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Mitigation \n",
    "Now, let's apply mitigation strategies to reduce the detected discrimination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying reweighting strategy...\")\n",
    "reweighted_data = reweighting(train_data, best_clusters, sensitive_columns, outcome_column)\n",
    "\n",
    "# Visualize weight distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reweighted_data['weight'], bins=30)\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Weights after Reweighting')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model with fairness regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with fairness regularization\n",
    "print(\"Training model with fairness regularization...\")\n",
    "fair_model = FairnessRegularizedModel(lambda_param=1.0)\n",
    "fair_model.fit(X_train, y_train, sensitive_train, best_clusters, nonsensitive_columns)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = fair_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply subgroup-specific calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply subgroup calibration\n",
    "print(\"Applying subgroup-specific calibration...\")\n",
    "# First make sure the outcome column is explicitly identified\n",
    "outcome_column_name = y_train.name if hasattr(y_train, 'name') else outcome_column\n",
    "\n",
    "# Make sure train_data has all the necessary columns\n",
    "train_data_for_calibration = train_data.copy()\n",
    "if 'cluster' not in train_data_for_calibration.columns:\n",
    "    train_data_for_calibration['cluster'] = best_clusters\n",
    "\n",
    "try:\n",
    "    calibration_results = subgroup_calibration(\n",
    "        train_data_for_calibration, \n",
    "        best_clusters, \n",
    "        sensitive_columns,\n",
    "        # You can optionally provide a base model here\n",
    "        base_model=None\n",
    "    )\n",
    "    \n",
    "    # Print performances by subgroup\n",
    "    print(\"\\nCalibration performance by subgroup:\")\n",
    "    \n",
    "    if 'performance' in calibration_results and calibration_results['performance']:\n",
    "        performances = calibration_results['performance']\n",
    "        \n",
    "        # Sort by improvement\n",
    "        improvements = [(key, perf['calibrated_acc'] - perf['base_acc']) \n",
    "                       for key, perf in performances.items()]\n",
    "        improvements.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for key, improvement in improvements:\n",
    "            perf = performances[key]\n",
    "            print(f\"  {key}: {perf['base_acc']:.4f} -> {perf['calibrated_acc']:.4f} \" + \n",
    "                  f\"({improvement*100:+.2f}%, {perf['samples']} samples)\")\n",
    "    else:\n",
    "        print(\"No performance metrics available. This may be due to insufficient samples in subgroups.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during calibration: {str(e)}\")\n",
    "    print(\"\\nFalling back to simple model comparison without calibration...\")\n",
    "    \n",
    "    # Train a regular model for comparison\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    regular_model = LogisticRegression(max_iter=1000)\n",
    "    regular_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Train a weighted model using reweighting\n",
    "    weighted_data = reweighting(train_data, best_clusters, sensitive_columns, outcome_column_name)\n",
    "    weighted_model = LogisticRegression(max_iter=1000)\n",
    "    weighted_model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        sample_weight=weighted_data['weight'].values if 'weight' in weighted_data.columns else None\n",
    "    )\n",
    "    \n",
    "    # Compare on test set\n",
    "    regular_preds = regular_model.predict(X_test)\n",
    "    weighted_preds = weighted_model.predict(X_test)\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    print(f\"Regular model accuracy: {accuracy_score(y_test, regular_preds):.4f}\")\n",
    "    print(f\"Weighted model accuracy: {accuracy_score(y_test, weighted_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluate Mitigation Effectiveness\n",
    "Finally, let's evaluate how effective our mitigation strategies were at reducing discrimination:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data with original and fair predictions\n",
    "test_data_with_clusters = test_data.copy()\n",
    "\n",
    "# Get clusters for test data (using nearest centroid)\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "centroid_classifier = NearestCentroid()\n",
    "centroid_classifier.fit(X_train.values, best_clusters)\n",
    "test_clusters = centroid_classifier.predict(X_test.values)\n",
    "test_data_with_clusters['cluster'] = test_clusters\n",
    "\n",
    "# Get predictions from a regular model for comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "regular_model = LogisticRegression(max_iter=1000)\n",
    "regular_model.fit(X_train, y_train)\n",
    "regular_preds = regular_model.predict(X_test)\n",
    "\n",
    "# Create datasets with predictions\n",
    "test_regular = test_data_with_clusters.copy()\n",
    "test_regular['prediction'] = regular_preds\n",
    "\n",
    "test_fair = test_data_with_clusters.copy()\n",
    "test_fair['prediction'] = y_pred\n",
    "\n",
    "# Calculate CMI for both prediction sets\n",
    "print(\"Evaluating mitigation effectiveness...\")\n",
    "regular_cmi = calculate_cmi(\n",
    "    test_regular,\n",
    "    test_clusters,\n",
    "    sensitive_columns,\n",
    "    'prediction',\n",
    "    nonsensitive_columns\n",
    ")\n",
    "\n",
    "fair_cmi = calculate_cmi(\n",
    "    test_fair,\n",
    "    test_clusters,\n",
    "    sensitive_columns,\n",
    "    'prediction',\n",
    "    nonsensitive_columns\n",
    ")\n",
    "\n",
    "print(f\"CMI on regular predictions: {regular_cmi:.4f}\")\n",
    "print(f\"CMI on fair predictions: {fair_cmi:.4f}\")\n",
    "print(f\"CMI reduction: {(1 - fair_cmi/regular_cmi)*100:.2f}%\")\n",
    "\n",
    "# Calculate accuracy\n",
    "regular_acc = accuracy_score(y_test, regular_preds)\n",
    "fair_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Regular model accuracy: {regular_acc:.4f}\")\n",
    "print(f\"Fair model accuracy: {fair_acc:.4f}\")\n",
    "print(f\"Accuracy change: {(fair_acc - regular_acc)*100:+.2f}%\")\n",
    "\n",
    "# Calculate CMI per cluster\n",
    "regular_cmi_per_cluster = calculate_cmi_per_cluster(\n",
    "    test_regular,\n",
    "    test_clusters,\n",
    "    sensitive_columns,\n",
    "    'prediction',\n",
    "    nonsensitive_columns\n",
    ")\n",
    "\n",
    "fair_cmi_per_cluster = calculate_cmi_per_cluster(\n",
    "    test_fair,\n",
    "    test_clusters,\n",
    "    sensitive_columns,\n",
    "    'prediction',\n",
    "    nonsensitive_columns\n",
    ")\n",
    "\n",
    "# Compare CMI reduction across clusters\n",
    "print(\"\\nCMI reduction by cluster:\")\n",
    "for cluster in sorted(regular_cmi_per_cluster.keys()):\n",
    "    if cluster in fair_cmi_per_cluster:\n",
    "        reg_cmi = regular_cmi_per_cluster[cluster]\n",
    "        fair_cmi = fair_cmi_per_cluster[cluster]\n",
    "        reduction = (1 - fair_cmi/reg_cmi)*100\n",
    "        print(f\"  Cluster {cluster}: {reg_cmi:.4f} -> {fair_cmi:.4f} ({reduction:.2f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusions\n",
    "Let's summarize our findings and draw conclusions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of our findings\n",
    "print(\"Summary of Discrimination Detection Analysis:\")\n",
    "print(f\"1. Optimal number of clusters: {optimal_k}\")\n",
    "print(f\"2. Best algorithm: {best_algorithm}\")\n",
    "print(f\"3. Overall CMI: {cmi_values[best_algorithm]:.4f}\")\n",
    "print(f\"4. High discrimination clusters: {high_discrim_clusters}\")\n",
    "\n",
    "if high_discrim_clusters:\n",
    "    # Get highest contributing attribute for each high-discrimination cluster\n",
    "    for cluster_id in high_discrim_clusters:\n",
    "        cluster_data = train_data_with_clusters[train_data_with_clusters['cluster'] == cluster_id]\n",
    "        \n",
    "        contributions = hierarchical_cmi_decomposition(\n",
    "            cluster_data,\n",
    "            np.zeros(len(cluster_data)),\n",
    "            sensitive_columns,\n",
    "            outcome_column,\n",
    "            nonsensitive_columns\n",
    "        )\n",
    "        \n",
    "        main_contributor = max(contributions.items(), key=lambda x: x[1])[0]\n",
    "        print(f\"5. Cluster {cluster_id} main contributor: {main_contributor}\")\n",
    "        \n",
    "        if len(sensitive_columns) >= 2:\n",
    "            interaction = interaction_information(\n",
    "                cluster_data,\n",
    "                np.zeros(len(cluster_data)),\n",
    "                sensitive_columns[0],\n",
    "                sensitive_columns[1],\n",
    "                outcome_column,\n",
    "                nonsensitive_columns\n",
    "            )\n",
    "            \n",
    "            effect_type = \"Synergistic\" if interaction > 0 else \"Redundant\"\n",
    "            print(f\"6. {effect_type} effect between {sensitive_columns[0]} and {sensitive_columns[1]}\")\n",
    "\n",
    "print(f\"7. Mitigation: {(1 - fair_cmi/regular_cmi)*100:.2f}% CMI reduction with {(fair_acc - regular_acc)*100:+.2f}% accuracy change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis reveals hidden patterns of discrimination in the Adult dataset that might not be evident from simple aggregate statistics. By using the clustering-based approach with Conditional Mutual Information, we identified specific clusters where discrimination is more pronounced.\n",
    "Our mitigation strategies successfully reduced this discrimination while maintaining prediction accuracy. The results demonstrate that it's possible to develop fairer automated decision-making systems without compromising performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}